---
title: "Tarea 8: regularización y selección de modelos"
output: html_notebook
---

En este ejemplo hacemos *análisis de sentimiento*, intentanto
predecir si reseñas de películas son positivas o negativas
a partir del texto de las reseñas. En este ejemplo
veremos un enfoque relativamente simple, que consiste en
considerar solamente las palabras que contienen las reseñas, sin tomar en
cuenta el orden (el modelo de bolsa de palabras o *bag of words*).

Usaremos regresión logística regularizada.

## Feature engineering básico 

Hay muchas maneras de preprocesar los datos para obtener
variables numéricas a partir del texto. En este caso simplemente
tomamos las palabras que ocurren más frecuentemente. 

- Encontramos las 3000 palabras más frecuentes sobre todos los textos, por ejemplo. 
Estas palabras son nuestro **vocabulario**.
- Registramos en qué documentos ocurre cada una de esas palabras.
- Cada palabra es una columna de nuestros datos, el valor es 1 si la palabra
ocurre en documento y 0 si no ocurre.


Por ejemplo, para el texto "Un gato blanco, un gato negro", "un perro juega", "un astronauta juega" quedarían los datos:

|texto_id | un | gato | negro | blanco | perro | juega |
-----|------|-------|--------|-------|-------  | ---- |
| texto_1 | 1  |  1   |   1   |   1    |  0    | 0     |
| texto_2 | 1  |  0   |  0    | 0      |  1    |  0
| texto_3 | 1  |  0   |  0    | 0      |  0    |  1   |

Nótese que la palabra *astronauta* no está en nuestro vocabulario para este ejemplo.


Hay varias opciones para tener mejores variables, que pueden o no ayudar en este
problema (no las exploramos en este ejercicio):

- Usar conteos de frecuencias de ocurrencia de 
palabras en cada documento, o usar log(1+ conteo), en lugar
de 0-1's
- Usar palabras frecuentes, pero quitar las que son *stopwords*,
como son preposiciones y artículos entre otras, pues no tienen significado: en inglés, por ejemplo, *so, is, then, the, a*, etc.
- Lematizar palabras: por ejemplo, contar en la misma categoría *movie* y *movies*, o
*funny* y *funniest*, etc.
- Usar indicadores binarios si la palabra ocurre o no en lugar de la frecuencia
- Usar frecuencias ponderadas por qué tan rara es una palabra sobre todos los documentos (frecuencia inversa sobre documentos)
- Usar pares de palabras en lugar de palabras sueltas: por ejemplo: juntar "not" con la palabra que sigue (en lugar de usar *not* y *bad* por separado, juntar en una palabra *not_bad*),
- Usar técnicas de reducción de dimensionalidad que considera la co-ocurrencia de palabras (veremos más adelante en el curso).
- Muchas otras

### Datos y preprocesamiento

Los textos originales los puedes encontrarlos en la carpeta *datos/sentiment*. 
Están en archivos individuales que tenemos que leer. Podemos hacer lo que sigue:

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(tidymodels)
# puedes necesitar el siguiente paquete
# install.packages("textrecipes")
# install.packages("stopwords")
nombres_neg <- list.files("../datos/sentiment/neg", full.names = TRUE)
nombres_pos <- list.files("../datos/sentiment/pos", full.names = TRUE)
# positivo
textos_pos <- tibble(texto = map_chr(nombres_pos, read_file), polaridad = "pos")
textos_neg <- tibble(texto = map_chr(nombres_neg, read_file), polaridad = "neg")
textos <- bind_rows(textos_pos, textos_neg) %>% 
  mutate(polaridad = factor(polaridad, levels = c("pos", "neg")))
nrow(textos)
table(textos$polaridad)
```

Y un fragmento del primer texto:

```{r}
str_sub(textos$texto[[5]], 1, 300)
```

```{r}
set.seed(83224)
polaridad_particion <- initial_split(textos, 0.7)
textos_ent <- training(polaridad_particion)
textos_pr <- testing(polaridad_particion)
nrow(textos_ent)
```


```{r}
# install.packages("textrecipes")
# install.packages("stopwords")
library(textrecipes)
receta_polaridad <- recipe(polaridad ~ ., textos_ent) %>%
  step_relevel(polaridad, ref_level = "neg", skip = TRUE) %>% 
  step_mutate(texto = str_remove_all(texto, "[_()]")) %>% 
  step_mutate(texto = str_remove_all(texto, "[0-9]*")) %>% 
  step_tokenize(texto) %>% # separar por palabras
  step_stopwords(texto) %>% 
  step_tokenfilter(texto, max_tokens = 6000) %>% 
  step_tf(texto, weight_scheme = "raw count") 

# en el prep se separa en palabras, se eliminan stopwords,
# se filtran los de menor frecuencia y se crean las variables
# 0 - 1 que discutimos arriba, todo con los textos de entrenamiento
receta_prep <- receta_polaridad %>% prep()
```


Los términos seleccionados (el vocabulario) están aquí (una muestra)

```{r}
receta_prep$term_info %>% sample_n(30)
```
El tamaño de la matriz que usaremos para regresión logística tiene 1600
renglones (textos) por 3000 columnas de términos:

```{r}
mat_textos_entrena <- juice(receta_prep) 
dim(mat_textos_entrena)
head(mat_textos_entrena)
```




## Clasificador de textos

Ahora hacemos regresión logística con regularización ridge/lasso. La penalización
es de la forma

$$\lambda((1-\alpha) \sum_j \beta_j^2 + \alpha \sum_j |\beta_j|)$$
de manera que combina ventajas de ridge (encoger juntos parámetros de variables
correlacionadas) y lasso (eliminar variables que aportan poco a la predicción).

Seleccionaremos los parámetros con validación cruzada

*Preguntas: *
1. Calcula la devianza validación cruzada y el area bajo la curva ROC  
para un modelo
poco regularizado (por ejemplo, $\log(\lambda) = -12$). 


```{r}
textos_pr <- testing(polaridad_particion)
modelo_baja_reg <- logistic_reg(mixture = 0.5, penalty = exp(-12)) %>% 
  set_engine("glmnet") %>% 
  set_args(lambda.min.ratio = 0)
flujo_textos <- workflow() %>% 
  add_recipe(receta_polaridad) %>% 
  add_model(modelo_baja_reg) %>% 
  fit(textos_ent)
preds_baja_reg <- predict(flujo_textos, textos_pr, type = "prob") %>% 
  bind_cols(textos_pr %>% select(polaridad)) 
preds_baja_reg %>% 
  mn_log_loss(factor(polaridad), .pred_pos)
predict(flujo_textos, textos_ent, type = "prob") %>% 
  bind_cols(textos_ent %>% select(polaridad)) %>% 
  mn_log_loss(factor(polaridad), .pred_pos)
```








La gráfica de precisión recall es:

```{r}
autoplot(preds_baja_reg %>% pr_curve(polaridad, .pred_pos))
```

**Pregunta**: Este modelo tiene sobreajuste fuerte. ¿Por qué?

2. Selecciona un modelo con regularización mayor:

```{r}
modelo_mas_reg <- logistic_reg(mixture = 0.5, penalty = 0.01) %>% 
  set_engine("glmnet") %>% 
  set_args(lambda.min.ratio = 0) 
flujo_textos_alta <- workflow() %>% 
  add_recipe(receta_polaridad) %>% 
  add_model(modelo_mas_reg) %>% 
  fit(textos_ent)
preds_alta_reg <- predict(flujo_textos_alta, textos_pr, type = "prob") %>% 
  bind_cols(textos_pr %>% select(polaridad)) 
preds_alta_reg %>% 
  mn_log_loss(polaridad, .pred_pos)
```

**Pregunta**: 
3. Grafica juntas las curvas de precisión recall. ¿Algún modelo domina a otro?
compara desempeño de los dos clasificadores en pérdida logarítmica.

```{r}

```

***Pregunta**
4. Obtén los coeficientes de los dos modelos que comparaste arriba. Compara los
coeficientes más negativos y más positivos de cada modelo. ¿Cuáles tienen
valores más grandes en valor absoluto? ¿Por qué? ¿Tiene sentido cuáles palabras
tienen coeficiente positivo y cuáles negativo?

```{r}
#por ejemplo:
coefs_baja <- flujo_textos %>% pull_workflow_fit() %>% tidy 
coefs_alta <- flujo_textos_alta %>% pull_workflow_fit() %>% tidy 
coefs_2mod <- bind_rows(coefs_baja, coefs_alta) %>% 
  mutate(tipo = ifelse(penalty < 0.0001, "coef_reg_baja", "coef_reg_alta")) %>% 
  select(term, tipo, estimate) %>% 
  pivot_wider(names_from = tipo, values_from = estimate)

```

**Pregunta **: 
5. Grafica coeficientes de un modelo contra los
del otro modelo (agrega una recta y=x). ¿Cómo describes los patrones que ves en esa gŕafica?



**Pregunta**:
6. Calcula cuántas de las predicciones tienen probabilidad menor
a 0.01 en el modelo sobreajustado. Entre esas predicciones, ¿cuál es la probabilidad
de que una reseña sea de hecho positiva (según la muestra de prueba)? 
Describe por qué esto explica en parte que la
devianza sea tan grande para el modelo 
no regularizado comparado con el regularizado.



7. Afina los parámetros *mixture* y *penalty* usando validación cruzada con el siguiente
código. Este proceso tarda más pues hay que ajustar varios modelos (puede paralelizarse).


```{r}
# alpha es mixture, y lambda es penalty
modelo_reg <- logistic_reg(mixture = tune(), penalty = tune()) %>% 
  set_mode("classification") %>%
  set_engine("glmnet") %>% 
  set_args(lambda.min.ratio = 0) 
flujo_textos <- workflow() %>% 
  add_recipe(receta_polaridad) %>% 
  add_model(modelo_reg)
```


```{r}
# cortes de validación cruzada
particion_vc <- vfold_cv(textos_ent, v = 5)
# crea un grid (empieza con uno chico)
glmnet_set <- parameters(penalty(range = c(-4, 0), trans = log10_trans()),
                         mixture(range = c(0, 1)))
glmnet_grid <- grid_regular(glmnet_set, levels = 10) 
# afinar
glmnet_tune <- tune_grid(flujo_textos,
            resamples = particion_vc,
            grid = glmnet_grid,
            metrics = metric_set(roc_auc, mn_log_loss))
res <- collect_metrics(glmnet_tune)
res
ggplot(res %>% filter(.metric == "mn_log_loss"), aes(x = penalty, y = mean, colour = mixture, group = mixture)) +
  geom_point() + geom_line() + scale_x_log10() 
```

Escogemos el modelo más simple a un error estándar del mejor:

```{r}
select_by_one_std_err(glmnet_tune, metric = "mn_log_loss", desc(penalty))
```

Podemos seleccionar modelos comparables con mayor penalización lasso para obtener
modelos más parsimoniosos.

